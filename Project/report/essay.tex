\documentclass{article}

\usepackage[final]{neurips_2019}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{float}   % 放在导言区
\usepackage{array}


\title{What If Without the Conformal Prediction?}

\author{
  Jing LIANG\\
  Industrial Engineering and Decision Analytics\\
  Hong Kong University of Science and Technology\\
  \texttt{jliangcd@connect.ust.hk} \\
}

\begin{document}

\maketitle

\begin{abstract}
  To be continued...
  % The abstract paragraph should be indented \nicefrac{1}{2}~inch (3~picas) on
  % both the left- and right-hand margins. Use 10~point type, with a vertical
  % spacing (leading) of 11~points.  The word \textbf{Abstract} must be centered,
  % bold, and in point size 12. Two line spaces precede the abstract. The abstract
  % must be limited to one paragraph.
\end{abstract}

\section{Introduction}

With the growing use of machine learning in risk-sensitive domains such as medical diagnosis and financial risk management, aggregate metrics such as average accuracy are no longer sufficient. Conventional learning algorithms such as neural networks, support vector machines, and decision trees typically provide only point predictions and do not offer calibrated measures of uncertainty. In practice, decision-makers require not only accurate predictions but also a principled assessment of their reliability. Conformal prediction meets this requirement by providing instance-wise, distribution-free validity guarantees that quantify the uncertainty associated with each prediction.

If conformal prediction did not exist, what we would lose is not merely an algorithm but a critical meta-methodology: a systematic framework that decouples uncertainty quantification from model design through the construction of nonconformity scores. The impact would extend beyond the disappearance of a single technique, representing a fundamental shift in the paradigm of algorithm design.


% This paper examines this question from multiple perspectives, including the historical development of conformal prediction, its core insights, and its distinctive role in modern machine learning. Sections 2 and 3 provide an in-depth overview of its evolution and underlying principles, clarifying its unique value and highlighting the essential differences from alternative approaches to uncertainty quantification. Section 4 presents a comparative analysis between conformal prediction and other uncertainty quantification methods, demonstrating its advantages and irreplaceability. Section 5 discusses its practical significance in real-world applications and explores the potential consequences for research and practice in a hypothetical scenario where conformal prediction does not exist, including the research directions that would be absent in its absence.
%这部分要根据文章结构调整一下


\section{Background and Need}



With the rapid development of artificial intelligence and its widespread deployment across society, both industry and government agencies are increasingly adopting AI-based decision-support systems and agentic task-execution systems. These systems are typically built upon predictive machine learning models, which generate predictions or recommended actions through supervised learning tasks\cite{SocialImpactAI}.

At the same time, AI technologies are accelerating their penetration into high-risk domains such as medical diagnosis, biometrics, face recognition, nuclear fusion, and industrial diagnostics\cite{Vovk2014conformal}. In these settings, model failures can lead to severe medical, ethical, economic, or safety consequences. As a result, society and regulatory bodies have raised expectations regarding the reliability, fairness, transparency, and accountability of AI systems, thereby driving the development of Trustworthy AI (TAI).

Within the TAI framework, uncertainty quantification (UQ) plays a central role in ensuring reliability. The goal of UQ is to characterize and evaluate various sources of uncertainty in predictive models, providing reliable and calibrated confidence information\cite{smith2024uncertainty}. This enables decision-makers to understand the trust boundaries of model outputs and make robust decisions accordingly.

Traditional UQ approaches face structural limitations that constrain their effectiveness in high-risk applications. Bayesian methods, such as Bayesian neural networks, provide probabilistic predictions but rely heavily on strong prior assumptions and the correctness of the model specification; inference is also computationally expensive for large-scale models. Frequentist methods, such as bootstrapping or dropout-based UQ, estimate uncertainty through resampling or approximate inference but typically lack distribution-free, finite-sample guarantees. They also become unstable under distribution shift and incur high computational costs.

Consequently, traditional UQ methods generally fail to provide instance-wise, distribution-free guarantees of validity.

Conformal prediction, however, offers a systematic solution. It provides verifiable, finite-sample validity guarantees for each individual instance without requiring any distributional assumptions, enabling precise quantification of predictive uncertainty.


\section{Conformal Prediction}
\subsection{Historical Development}

The theoretical origins of conformal prediction were established in the late 1990s and early 2000s by Vladimir Vovk, Alex Gammerman, and Glenn Shafer\cite{OriginTrasnduction,ICM,vovk2005conformal}. Unlike traditional parametric statistics or Bayesian inference, the methodology arises from the frameworks of algorithmic randomness and game-theoretic probability\cite{vovk2005conformal,Gammerman1999Kolmogorov}. Its objective is to construct distribution-free predictive guarantees for machine learning that remain valid for finite samples. Within this paradigm, predictions produce sets of plausible labels rather than single point estimates, and the size of these sets adapts to model uncertainty, thereby providing a direct quantification of predictive reliability.


The foundational contribution to the field is the work that establishes the theoretical basis of conformal prediction \citep{vovk2005conformal}. A more recent overview is provided by the monograph “Conformal prediction for reliable machine learning: theory, adaptations and applications” \citep{Vovk2014conformal}.

Recent developments in conformal prediction have focused on three main areas: improving the validity concept, enhancing computational efficiency, and expanding application scope\citep{fontana2023conformal}.

% To achieve a stronger guarantee than basic marginal coverage, methods like Mondrian CP (MCPs) were introduced to ensure conditional validity within defined data categories, and Conformalized Quantile Regression (CQR) was developed to generate adaptive prediction intervals for heteroscedastic data. In terms of efficiency, the Inductive CP (ICP) framework has become dominant . ICP splits data into training and calibration sets, dramatically lowering the computation cost compared to the original transductive CP. More advanced versions, such as Jackknife+ and Cross-conformal prediction, further enhance stability and data utilization. Finally, CP's applicability has been extended to complex scenarios, including Functional Data Analysis (FDA) for function prediction bands, as well as developing robust extensions for non-i.i.d. data like time series and situations involving covariate shift.
%先注释掉，如果后面的内容感觉不太够的话这里再凑一点工作量

\subsection{Conformal Prediction Framework}
Vovk’s work introduces full conformal prediction or transductive conformal prediction \cite{vovk2005conformal}. The central idea is that, given a training set and a new test instance, each possible label for the test instance is tentatively paired with the training data to compute a nonconformity score, and the label is included in the prediction set based on its rank among these scores. Because this procedure becomes computationally expensive as the dataset grows, subsequent research has proposed more efficient variants, most notably split conformal prediction\cite{SplitConformalPrediction}, which is now widely used. This subsection will focus on split conformal prediction and its technical details, and will illustrate the overall conformal framework by comparing the associated forms of variance.


%这里抄一下2018的论文\cite{SplitConformalPrediction}

% A simple introduction to the conformal prediction algorithm

% 1. Core insight: a revolutionary abstraction of the “score” definition
%    Your observation that “the definition of the score removes the dependence on the model’s internal structure” captures the essence of conformal prediction.


\section{Key Insights and Benefit of Conformal Prediction}

(1) The key step: the nonconformity score
• Traditional view: model outputs (such as softmax probabilities or SVM decision values) are used directly as proxies for confidence. These are tightly coupled to the model’s internal structure. A neural network probability and a random forest probability do not have the same meaning and cannot be compared directly.
• CP’s abstraction: it introduces a nonconformity score A(x, y). The only requirement is that for a data point (x, y), a higher score means the pair looks more “strange” or “nonconforming” relative to the model or data distribution.
• Structure removed: this score A can be anything—prediction error |y  f(x)|, negative log-likelihood, distance to a decision boundary, or variance across an ensemble. CP does not care how A is constructed; it only uses the ranking of these scores. It compresses any model into a scalar score generator.

(2) Significance: this abstraction achieves model-agnosticism. Whether using a ResNet, a transformer, or logistic regression, as long as a “strangeness score” can be computed for each candidate label, CP can provide coverage guarantees. This decouples uncertainty quantification from model design.

2. Theoretical foundation: trading “exchangeability” for mathematical guarantees
   Your point that CP “uses exchangeability to obtain mathematical guarantees” touches its statistical core.

(1) Mild assumption: CP does not require the data to be i.i.d.; it only needs exchangeability—the joint distribution remains unchanged under index permutations. This is weaker than i.i.d. and closer to practical settings (e.g., time series can often be made approximately exchangeable after proper preprocessing).
(2) Strong guarantee: under this mild assumption, CP yields its key validity guarantee: for any new test point, the probability that the true label falls outside the prediction set is at most , the preset error rate. This is a finite-sample, distribution-free guarantee independent of model complexity or data distribution.
(3) Insight: CP demonstrates a path where strong, practical statistical guarantees arise not from assuming a specific data-generating distribution, but from exploiting symmetry properties of data order. This is foundational for statistical inference in modern high-dimensional, complex data settings.




Therefore, the true insight of conformal prediction is that it offers a meta-methodology:

1. Decoupling: It separates model performance (implicitly reflected through the score function A) from output guarantees (derived from rank-based calibration and exchangeability).
2. Guarantee-first design: It reverses the traditional approach. Instead of trying to make model outputs “look like” probabilities and then hoping for guarantees, CP first builds a framework that inherently provides guarantees (via calibration and set-valued prediction), and then embeds any model inside it.
3. Infrastructure-friendly: It provides a simple, unified, and theoretically sound reliability layer for the diverse and ever-changing machine-learning ecosystem. This is not only an algorithmic innovation but also an elevation in system-level engineering thinking.

%最优性 It turns out that any valid invariant confidence predictor is a conformal predictor or can be improved to become a conformal predictor (Vovk et al., 2005, , Theorem 2.6).

% \section{Introduction about Conformal Prediction}
% What if the world had no conformal prediction? This paper attempts to answer that question.
% But before addressing this question, we must first introduce the unique nature of the conformal prediction topic itself.
% Compared to algorithms like gradient boosting that have been around for some time, conformal prediction—proposed in the 2000s and gaining prominence only recently—is a very young research direction. Therefore, beyond merely discussing what the present would be like without conformal prediction, we should also reflect on the algorithm's future impact by examining the insights it offers. If this research direction had never been proposed, how might related theories and applications evolve in the future?


% ovk 在1980–1990年代深入研究了算法随机性的数学基础，提出了“超鞅”（supermartingales）的概念来形式化检测序列中的非随机性或非交换性。他证明，在一个公平的博弈中，如果数据序列存在某种模式或偏差，一个精明的赌徒可以通过构造特定的超鞅策略使其资本无界增长。这一思想后来被直接应用于共形预测中：当一个新的测试样本到来时，通过将其与训练集合并并计算每个可能的标签所对应的“非共形测度”（nonconformity measure），可以构造出一系列p值。这些p值本质上反映了该样本在当前假设下有多“异常”或“不共形”，其计算过程正是基于数据排列的对称性，从而天然地依赖于交换性假设而非i.i.d.

\section{Alternative Methods}

A wide range of uncertainty quantification techniques have been developed across statistics and machine learning. These approaches differ in their underlying assumptions, the form of guarantees they provide, and their suitability for various application settings. Table~\ref{tab:cp-competitors} summarizes the main methodological families commonly used as competitors to conformal prediction.

\begin{table}[H]
\centering
\begin{tabular}{
    >{\raggedright\arraybackslash}p{3.5cm}
    >{\raggedright\arraybackslash}p{4cm}
    >{\raggedright\arraybackslash}p{5cm}
}
\toprule
\textbf{Method Family} & \textbf{Key Idea} & \textbf{Representative Competitors} \\
\midrule

\textbf{Classical parametric prediction intervals} 
& Assume a fully specified parametric model, typically linear with Gaussian noise
& Gaussian linear model (t-interval) \\

\addlinespace

\textbf{Semi- / Non-parametric prediction bands} 
& Assume a functional form for estimation, such as conditional means or quantiles, without making distributional assumptions
& Quantile regression bands \\

\addlinespace

\textbf{Bayesian framework} 
& Characterize uncertainty through the posterior predictive distribution
& Bayesian neural networks; Bayesian model averaging \\

\addlinespace

\textbf{PAC-learning} 
& Provide worst-case upper bounds on prediction error over the hypothesis class
& Vapnik–Chervonenkis bounds; Littlestone–Warmuth bounds \\

\addlinespace

\textbf{Hold-out and resampling-based methods} 
& Estimate uncertainty empirically via data splitting or repeated resampling
& Train–test hold-out; cross-validation; bootstrap \\

\bottomrule
\end{tabular}
\caption{Major competitor methods to conformal prediction}
\label{tab:cp-competitors}
\end{table}

None of these approaches simultaneously achieve the combination of distribution-free operation, finite-sample validity, and instance-wise guarantees that conformal prediction provides. Nevertheless, each method family has its own advantages and suitable application contexts. The remainder of this section outlines the core ideas behind several representative competitors and contrasts them with conformal prediction to highlight its distinct value.

\subsection{Bayesian framework}
%先验不行的话就很糟糕




\subsection{Boostrap resampling}
% Bootstrap residual intervals
% 残差方差估计偏小 → 区间过窄 → 覆盖率不足

% 优点：通用
% 缺点：
% 残差方差普遍估小 → 区间过窄
% 对 heavy-tail/noise 弱
% 对 out-of-domain 样本失效
% 无法做 object-wise calibration

% 1. 计算量巨大……
% 哇所以boostrap是在训练阶段要训练B次，预测阶段要调用B个模型，B 通常是 200–2000。
% Conformal prediction 只需要训练一次模型，预测阶段只需要计算 n+1 次非拟合度分数排序就可以了，计算量小很多。

## 总体设定（所有实验共用）

* 回归模型：
  (X \sim \text{Unif}[-1,1])，若干不同的真模型 (f^*) 与噪声 (\varepsilon)。
* 目标：预测 (Y)，构造 95% 区间，比较：

  * Conformal（split CP，NCM = |residual|）
  * Bootstrap（residual bootstrap + 线性/NN 重拟合）
* 评价：

  * overall coverage：(\Pr{Y\in \hat C(X)})
  * 平均区间长度
  * 条件 coverage：在 x-bins 上估计 (\Pr{Y\in \hat C(X)\mid X\in \text{bin}})
  * 运行时间（可选）

下面按 claim 设计 4 个实验。

---

## 实验 1：模型正确 vs 错误 → “Bootstrap 在 misspecification 下失效，CP 始终 valid”

### 1A 模型正确（线性 + 高斯噪声）

* 真模型：
  (Y = 2X + \varepsilon,\ \varepsilon \sim N(0,1))
* 数据：

  * train: n_train = 200
  * cal: n_cal = 200（给 CP 用）
  * test: n_test = 5000
* 基模型：线性回归

方法：

* CP：

  * 在 train 上拟合线性模型，
  * 在 cal 上算 residuals (|Y_i - \hat f(X_i)|)，取 (1−α)(n_cal+1)-th quantile = q_hat
  * 区间：([\hat f(x) - q_{\text{hat}}, \hat f(x) + q_{\text{hat}}])
* Bootstrap：

  * 在 train 上拟合线性，得 residuals (\hat\varepsilon_i)
  * 重抽 B=500 组 residuals，在每次 bootstrap 上重构 (Y^*)，重拟合线性，得到 (\hat f_b(x)) 或 residual 分布
  * 取 bootstrap residual/预测分布的 2.5%–97.5% 形成区间

预期结果：

* 两者的 coverage 都接近 0.95
* Bootstrap 区间略短一点（高效），CP 稍长一点

这对应：**“模型正确时，Bayes/parametric/Bootstrap 比 CP 略更 efficient，但 CP 仍 valid”。**

---

### 1B 模型错误（非线性真模型 + 仍用线性）

* 真模型：
  (Y = 2X + 5X^2 + \varepsilon,\ \varepsilon \sim N(0,1))

* 其它设定同上（train/cal/test）

* 基模型：仍然用“线性回归”去拟合

方法同 1A。

预期现象：

* CP：coverage 仍 ≈ 0.95，但 avg length 明显变长
* Bootstrap：coverage 掉到比如 0.7–0.8，甚至更低，区间依然较短

这直接验证：

> “随着假设模型越不对，Bootstrap 失去 validity 但保持短；CP 保持 validity 但区间变宽”。

---

## 实验 2：heavy-tail / 异方差噪声 → “Bootstrap 依赖分布条件，CP 只要 exchangeability”

设计两种噪声：

### 2A heavy-tail

* 真模型：(Y = 2X + \varepsilon)
* 噪声：(\varepsilon \sim t_3)（或 t_2）
* 基模型仍线性回归

### 2B 异方差

* 真模型：
  (Y = 2X + \sigma(X)\varepsilon)
  (\varepsilon \sim N(0,1))，(\sigma(X) = 0.5 + |X|)（靠近 ±1 方差大）

其它设定同前。

方法：

* CP：像 1A，只是 residual 自然会反映 heavy-tail/heteroskedastic，quantile 变大
* Bootstrap：仍用 residual bootstrap（很多人默认使用）

预期：

* heavy-tail：

  * CP coverage ≈ 0.95，区间很宽
  * Bootstrap coverage 明显 <0.95，尤其 tail 部分 Y 被截断
* 异方差：

  * overall coverage 可以看起来还好，但**对大 |X| 的 bin**，Bootstrap coverage 会很低
  * CP 的 conditional coverage 更平滑一些（虽然仍然只是 marginal 保证）

这验证：

> “Bootstrap 对 heavy-tail/异方差敏感，finite-sample 下覆盖差；CP 只要 i.i.d./exchangeable 就有 finite-sample validity。”

---

## 实验 3：高维 + 黑箱模型 → “CP 可直接套在复杂估计器，Bootstrap 不稳”

### 设定

* X 维度：p = 20

  * 生成 X ~ N(0, Σ)，Σ_ij = 0.5^{|i-j|}
* 真模型：
  (Y = X_1 + X_2 + X_3 + \varepsilon,\ \varepsilon \sim N(0,1))
* n_train = 300, n_cal = 300, n_test = 5000
* 基模型：随机森林回归（或浅层 NN）

方法：

* CP：

  * 用 RF/NN 在 train 上拟合
  * 在 cal 上 residual → quantile
* Bootstrap：

  * 对 train 做 residual bootstrap：每次重抽 residual + 重新训练 RF/NN（昂贵且不稳定）
  * 用 bootstrap 预测分布取 2.5%–97.5%

记录：

* coverage
* length
* 时间（train+区间构造）

预期：

* high-dim + RF/NN 下：

  * CP 基本能保持 coverage ~ 0.95（只要 base model 不完全爆炸）
  * Bootstrap coverage 明显波动更大（模型拟合非常不稳定）
  * 计算时间：Bootstrap >> CP（尤其有 CV/复杂模型时）

这支撑：

> “在复杂/高维模型下，full parametric/Bootstrap 不再可靠，CP+split 非常实用且高效。”

---

## 实验 4：instance-wise vs global → “Bootstrap 给的是整体误差，CP 给单点置信信息”

选任意一个前面场景（比如 2B 异方差），在 test 上做：

1. 按 X 划分成若干 bin（例如 [-1,-0.5], [-0.5,0], [0,0.5], [0.5,1]）
2. 对每个 bin 计算：

   * CP 的 coverage_bin
   * Bootstrap 的 coverage_bin
   * 两者平均区间长度

同时再看 “instance-wise 尺度”：

* 对每个 test 点：

  * CP：p-value 或 interval size (|C(x)|) 作为“不确定性指示器”
  * Bootstrap：只能给一个“统一 nominal 95% 区间”，没有点级别置信度刻度

你可以画：

* x-bin vs coverage（两条曲线：CP / Bootstrap）
* x-bin vs mean interval length

预期：

* Bootstrap 在大噪声区域（大 |X|）coverage 崩掉
* CP 的 coverage 变化小得多
* CP 自然给每个样本一个 interval size / p-value，展示 instance-wise 信心的“分辨率”；Bootstrap 的“95% 区间”在全局是统一标称，没有 per-instance 区别。

---

## 你论文里可以怎么用

* Section “Experiments” 结构：

  1. Linear-Gaussian（1A）：“sanity check: CP vs Bootstrap under correct model”
  2. Nonlinear / heavy-tail / heteroskedastic（1B + 2）：“Bootstrap loses validity under misspecification; CP keeps finite-sample coverage, trades length”
  3. High-dimensional RF/NN（3）：“CP remains valid and efficient on complex black-boxes, Bootstrap unstable and expensive”
  4. Instance-wise analysis（4）：“Global vs instance-wise guarantees”

每个小节放一张表（coverage + length）+ 1–2 张关键图即可。







\subsection{quantile regression}
% 训练之后输出条件分位数，可以用这个来构建区间，但是无法保证覆盖率，也就是可信度时是下降的\cite{quantileregressionReview}
% 需要修改模型本身的loss function
% 但是现在有将两个结合起来的工作
% actually 现在有很多方法在把这两者结合起来


%需要做的实验有1. 验证模型错配情况下两种算法的表现，2给出有限样本下quantile regression的表现和conformalprediction的对比

\subsection{Others}
%随便写点吧，时间来不及就直接引用别人论文里面的内容







\section{Implications of a World Without Conformal Prediction}
TBC

\section{Conclusion}
TBC



\small
\bibliographystyle{unsrtnat}   % 按出现顺序排序
\bibliography{reference}     % 对应 references.bib 文件


\end{document}