\documentclass{article}

\usepackage[final]{neurips_2019}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\makeatletter
\renewcommand{\@noticestring}{}
\makeatother

\usepackage{float}   % 放在导言区
\usepackage{array}

\usepackage{graphicx}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{float}  % 需要这个包
\usepackage{amsmath}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}

\title{What If Without the Conformal Prediction?}

\author{
  Jing LIANG\\
  Industrial Engineering and Decision Analytics\\
  Hong Kong University of Science and Technology\\
  \texttt{jliangcd@connect.ust.hk} \\
}

\begin{document}

\maketitle

\begin{abstract}
Black-box predictive models are increasingly used in domains where reliable uncertainty quantification is essential. 
Conformal prediction provides a model-agnostic and distribution-free mechanism for constructing finite-sample valid prediction sets through score-based calibration under exchangeability. 
This paper examines what would be missing from the uncertainty quantification landscape in the absence of conformal prediction. The discussion outlines its core principles, representative variants, and theoretical structure, and contrasts these with several alternative approaches—including Bayesian inference, bootstrap resampling, and quantile regression. 
Empirical comparisons on synthetic and real datasets illustrate that these alternatives, while useful in specific settings, do not jointly offer finite-sample distribution-free validity and broad model compatibility. 
The analysis highlights the distinctive conceptual insights introduced by conformal prediction, as well as its practical value across diverse predictive applications.
\end{abstract}

\section{Introduction}

With the growing use of machine learning in risk-sensitive domains such as medical diagnosis and financial risk management, aggregate metrics such as average accuracy are no longer sufficient. Conventional learning algorithms such as neural networks, support vector machines, and decision trees typically provide only point predictions and do not offer calibrated measures of uncertainty. In practice, decision-makers require not only accurate predictions but also a principled assessment of their reliability.

Conformal prediction addresses this need by constructing, for each new input $X_{n+1}$, a prediction set
\[
  C_\alpha(X_{n+1}) \subseteq \mathcal{Y},
\]
where $\mathcal{Y}$ denotes the label space and $\alpha \in (0,1)$ is the target miscoverage level. The goal is to ensure the finite-sample, distribution-free validity guarantee
\[
  \mathbb{P}\!\left( Y_{n+1} \in C_\alpha(X_{n+1}) \right) \ge 1 - \alpha ,
\]
for the unknown true label $Y_{n+1}$, assuming only that the data $(X_i,Y_i)_{i=1}^{n+1}$ are exchangeable.

To achieve this, conformal prediction defines a \emph{nonconformity score}
\[
  V_i = V(X_i, Y_i), \qquad i = 1,\dots,n,
\]
which measures how atypical an example $(X_i,Y_i)$ is relative to the others. A candidate label $y$ for the new input $X_{n+1}$ is evaluated via its test-time score $V(X_{n+1}, y)$ and included in the prediction set whenever it does not exceed the appropriate empirical quantile of the calibration scores.

If conformal prediction did not exist, what we would lose is not merely an algorithm but a critical meta-methodology: a systematic framework that decouples uncertainty quantification from model design through the construction of nonconformity scores. The impact would extend beyond the disappearance of a single technique, altering the fundamental paradigm for developing uncertainty-aware predictive systems.

The source code for all experiments is publicly available at:
\url{https://github.com/Revelyn-Jing/HKUST-MATH-5472/tree/main/Project}.

% This paper examines this question from multiple perspectives, including the historical development of conformal prediction, its core insights, and its distinctive role in modern machine learning. Sections 2 and 3 provide an in-depth overview of its evolution and underlying principles, clarifying its unique value and highlighting the essential differences from alternative approaches to uncertainty quantification. Section 4 presents a comparative analysis between conformal prediction and other uncertainty quantification methods, demonstrating its advantages and irreplaceability. Section 5 discusses its practical significance in real-world applications and explores the potential consequences for research and practice in a hypothetical scenario where conformal prediction does not exist, including the research directions that would be absent in its absence.
%这部分要根据文章结构调整一下


\section{Background and Need}



With the rapid development of artificial intelligence and its widespread deployment across society, both industry and government agencies are increasingly adopting AI-based decision-support systems and agentic task-execution systems. These systems are typically built upon predictive machine learning models, which generate predictions or recommended actions through supervised learning tasks\cite{SocialImpactAI}.

At the same time, AI technologies are accelerating their penetration into high-risk domains such as medical diagnosis, biometrics, face recognition, nuclear fusion, and industrial diagnostics\cite{Vovk2014conformal}. In these settings, model failures can lead to severe medical, ethical, economic, or safety consequences. As a result, society and regulatory bodies have raised expectations regarding the reliability, fairness, transparency, and accountability of AI systems, thereby driving the development of Trustworthy AI (TAI).

Within the TAI framework, uncertainty quantification (UQ) plays a central role in ensuring reliability. The goal of UQ is to characterize and evaluate various sources of uncertainty in predictive models, providing reliable and calibrated confidence information\cite{smith2024uncertainty}. This enables decision-makers to understand the trust boundaries of model outputs and make robust decisions accordingly.

Traditional UQ approaches face structural limitations that constrain their effectiveness in high-risk applications. Bayesian methods, such as Bayesian neural networks, provide probabilistic predictions but rely heavily on strong prior assumptions and the correctness of the model specification; inference is also computationally expensive for large-scale models. Frequentist methods, such as bootstrapping or dropout-based UQ, estimate uncertainty through resampling or approximate inference but typically lack distribution-free, finite-sample guarantees. They also become unstable under distribution shift and incur high computational costs.

Consequently, traditional UQ methods generally fail to provide instance-wise, distribution-free guarantees of validity.

Conformal prediction, however, offers a systematic solution. It provides verifiable, finite-sample validity guarantees for each individual instance without requiring any distributional assumptions, enabling precise quantification of predictive uncertainty.


\section{Conformal Prediction}
\subsection{Historical Development}

The theoretical origins of conformal prediction were established in the late 1990s and early 2000s by Vladimir Vovk, Alex Gammerman, and Glenn Shafer\cite{OriginTrasnduction,ICM,vovk2005conformal}. Unlike traditional parametric statistics or Bayesian inference, the methodology arises from the frameworks of algorithmic randomness and game-theoretic probability\cite{vovk2005conformal,Gammerman1999Kolmogorov}. Its objective is to construct distribution-free predictive guarantees for machine learning that remain valid for finite samples. Within this paradigm, predictions produce sets of plausible labels rather than single point estimates, and the size of these sets adapts to model uncertainty, thereby providing a direct quantification of predictive reliability.


The foundational contribution to the field is the work that establishes the theoretical basis of conformal prediction \citep{vovk2005conformal}. A more recent overview is provided by the monograph “Conformal prediction for reliable machine learning: theory, adaptations and applications” \citep{Vovk2014conformal}.

Recent developments in conformal prediction have focused on three main areas: improving the validity concept, enhancing computational efficiency, and expanding application scope\citep{fontana2023conformal}.



\subsection{Conformal Prediction Framework}
Vovk’s work introduces full conformal prediction or transductive conformal prediction \cite{vovk2005conformal}. The central idea is that, given a training set and a new test instance, each possible label for the test instance is tentatively paired with the training data to compute a nonconformity score, and the label is included in the prediction set based on its rank among these scores. Because this procedure becomes computationally expensive as the dataset grows, subsequent research has proposed more efficient variants, most notably split conformal prediction\cite{SplitConformalPrediction}, which is now widely used. This subsection will focus on split conformal prediction and its technical details, and illustrate the overall conformal framework.

\subsubsection{Split Conformal Prediction}

Recall the targert of conformal prediction : given i.i.d.\ data $(X_i,Y_i)_{i=1}^n$ with $X_i\in\mathcal{X}$, $Y_i\in\mathcal{Y}$ and a target
miscoverage level $\alpha\in(0,1)$, conformal prediction attempts to construct, for
each new input $X_{n+1}$, a prediction set
\[
  C_{\alpha}(X_{n+1}) \subseteq \mathcal{Y}
\]
such that the coverage rate guarantee
\[
  \mathbb{P}\!\left( Y_{n+1} \in C_{\alpha}(X_{n+1}) \right) \ge 1-\alpha
\]
holds for any underlying distribution, assuming only exchangeability (i.i.d.) of
$(X_i,Y_i)_{i=1}^{n+1}$.


Split conformal prediction achieves this by partitioning the data into two parts: a
training subset for fitting an arbitrary base model $\hat f$, and a calibration subset for
quantifying the model’s residual uncertainty.  
The calibration samples provide an empirical distribution of nonconformity scores, whose
appropriate quantile is then used to expand the model’s raw prediction into a prediction
set with controlled miscoverage.  
In this way, split CP (Conformal Prediction) converts any point predictor into a distribution-free, finite-sample
valid prediction region.



\begin{algorithm}[H]
\caption{Split Conformal Prediction }
\label{alg:split-conformal}
\begin{algorithmic}[1]
\REQUIRE Dataset $\{(X_i, Y_i)\}_{i=1}^n$; base learner $\mathcal{A}$; nonconformity function $V_f$; miscoverage level $\alpha \in (0,1)$
\ENSURE Mapping $x \mapsto C_\alpha(x)$

\STATE Randomly split the index set $\{1,\dots,n\}$ into disjoint subsets:
       training indices $I_{\mathrm{train}}$ and calibration indices $I_{\mathrm{cal}}$

\STATE Fit the prediction model on the training set:
       \[
         f \leftarrow \mathcal{A}\bigl(\{(X_i, Y_i) : i \in I_{\mathrm{train}}\}\bigr)
       \]

\STATE For each $i \in I_{\mathrm{cal}}$, compute the nonconformity score:
       \[
         V_i \leftarrow V_f(X_i, Y_i), \qquad i \in I_{\mathrm{cal}}
       \]

\STATE Let $m \leftarrow |I_{\mathrm{cal}}|$ and
       $k \leftarrow \bigl\lceil (1-\alpha)(m+1)\bigr\rceil$.
       Define $\widehat{Q}_{1-\alpha}$ as the $k$-th smallest value in
       $\{V_i : i \in I_{\mathrm{cal}}\}$

\STATE For any new input $x \in \mathcal{X}$, define the prediction set:
       \[
         C_\alpha(x)
           \leftarrow \bigl\{y \in \mathcal{Y} : V_f(x,y) \le \widehat{Q}_{1-\alpha}\bigr\}
       \]

\end{algorithmic}
\end{algorithm}
Moreover, the split conformal prediction algorithm comes with a rigorous theoretical coverage guarantee.

\begin{theorem}[Conformal coverage guarantee for Split CP\cite{angelopoulos2021gentle}]
Suppose the observations $(X_i,Y_i)_{i=1,\ldots,n}$ and the test pair
$(X_{\mathrm{test}},Y_{\mathrm{test}})$ are i.i.d.\ and let
$\widehat{Q}_{1-\alpha}$ and $C_\alpha(X_{\mathrm{test}})$ be defined as in
Algorithm~\ref{alg:split-conformal}. Then the following coverage guarantee holds:
\[
  \mathbb{P}\!\left( Y_{\mathrm{test}} \in C_\alpha(X_{\mathrm{test}}) \right)
  \;\ge\; 1-\alpha.
\]
\end{theorem}

Notice that this theorem shows that the coverage guarantee of split CP is not asymptotic but holds non-asymptotically for any sample size.

\subsubsection{Variancs and Shared Framework}
In recent years, conformal prediction has attracted considerable attention and has advanced rapidly. In the following, we provide several representative variants and explain how the components that vary, as well as those that remain invariant, help reveal the essential structure of the conformal prediction framework.


\begin{table}[t]
\centering
\begin{tabular}{
    >{\raggedright\arraybackslash}p{2.5cm}
    >{\raggedright\arraybackslash}p{3.5cm}
    >{\raggedright\arraybackslash}p{6.5cm}
}
\toprule
\textbf{Method} & \textbf{Objective} & \textbf{Core Idea} \\
\midrule

CV+ / Jackknife+ \cite{jackknifePlus}
& Improve statistical efficiency and avoid overly wide intervals from data splitting 
& Use cross-validation or leave-one-out residuals to obtain more stable error estimates and produce tighter prediction sets. \\

\addlinespace

Weighted CP \cite{weightedConformal}
& Achieve valid coverage under covariate shift or population heterogeneity 
& Rank scores using importance weights that reflect differences between the target and calibration distributions. \\

\addlinespace

Distributional CP \cite{distributionalConformal}
& Handle complex, multimodal, or asymmetric conditional distributions 
& Construct scores based on estimated CDF values (e.g., $|F(x,y)-0.5|$) to form high-density prediction regions. \\


\addlinespace

CP for hierarchical structure data \cite{hierarchicalConformal}
& Handle data with group-level heterogeneity and hierarchical sampling structures
& Replace standard exchangeability with hierarchical exchangeability and construct prediction sets using pooled CDF, double conformal, or subsampling-based aggregation across groups. \\

\addlinespace

Adaptive Prediction Sets\cite{romano2020classification,angelopoulos2020uncertainty}
& Improve fairness of coverage across classes and sample difficulty 
& Use cumulative softmax probability to define an ordered score and adjust prediction set size according to sample difficulty. \\

\addlinespace

Conformalized Quantile Regression\cite{CQR}
& Construct more adaptive regression intervals 
& Measure deviation from upper and lower quantiles and calibrate these to form intervals adaptive to conditional distribution shape. \\


\bottomrule
\end{tabular}
\end{table}


From the table, we can see that although recent developments in conformal prediction have produced many variants, their innovations mainly focus on how the score is designed, what is calibrated, or how the exchangeability assumption is extended.
These represent the variant component of the CP framework. Different methods modify the source of the scores, the geometric form of the scores, or model the underlying data structure and distribution shift to better adapt to new tasks, data structures, or distributional settings. These changes improve the applicability of CP in complex, high-dimensional, heteroscedastic, multimodal, and non-i.i.d. scenarios, allowing the framework to remain flexible across different use cases.


In contrast, all methods preserve the invariant component of conformal prediction. No matter how the method varies, the validity mechanism follows the same basic design: construct a score that measures the discrepancy between the prediction and the ground truth, compute its empirical distribution on the calibration set, and rely on exchangeability to obtain validity guarantees.

\section{Key Insights and Benefit of Conformal Prediction}
From Section~3.2.2, it is clear that although conformal prediction admits many variants, they all follow the same fundamental framework, whose core lies in the construction of the score function and the associated calibration mechanism.

The distribution-free property originates from the design of the score: model adequacy is entirely captured by the nonconformity measure \(A(\cdot)\), while the coverage guarantee is provided solely by rank-based calibration under exchangeability. These two components are decoupled, allowing any black-box predictor to obtain formal validity without modifying its internal structure.

Abstractly, a score can be viewed as \(1 - \text{p-value}\)\cite{vovk2005conformal}. Consequently, the remaining steps of conformal prediction amount to determining whether a new input behaves like a sample drawn from the empirical distribution of scores, and forming the prediction set accordingly. This procedure relies only on exchangeability to obtain marginal coverage guarantees, making conformal prediction one of the few methods that retain valid coverage even in finite-sample regimes.

In an era of increasingly complex models and highly specialized tasks, this structural and portable decoupling principle becomes especially important.


\section{Alternative Methods}

A wide range of uncertainty quantification techniques have been developed across statistics and machine learning. These approaches differ in their underlying assumptions, the form of guarantees they provide, and their suitability for various application settings. Table summarizes the main methodological families commonly used as competitors to conformal prediction.

\begin{table}[H]
\centering
\begin{tabular}{
    >{\raggedright\arraybackslash}p{3.5cm}
    >{\raggedright\arraybackslash}p{4cm}
    >{\raggedright\arraybackslash}p{5cm}
}
\toprule
\textbf{Method Family} & \textbf{Key Idea} & \textbf{Representative Competitors} \\
\midrule

\textbf{Classical parametric prediction intervals} 
& Assume a fully specified parametric model, typically linear with Gaussian noise
& Gaussian linear model (t-interval) \cite{casella2024statistical}\\

\addlinespace

\textbf{Semi- / Non-parametric prediction bands} 
& Assume a functional form for estimation, such as conditional means or quantiles, without making distributional assumptions
& Quantile regression \cite{koenker2001quantile}\\

\addlinespace

\textbf{Bayesian framework} 
& Characterize uncertainty through the posterior predictive distribution
& Bayesian neural networks\cite{BNN}; Empirical Bayesian \cite{EmpiricalBayesian}\\

\addlinespace

\textbf{PAC-learning} 
& Provide worst-case upper bounds on prediction error over the hypothesis class
& Vapnik–Chervonenkis bounds\cite{vapnik1999overview}; Littlestone–Warmuth bounds\cite{Littlestone_and_Warmuth} \\

\addlinespace

\textbf{Hold-out and resampling-based methods} 
& Estimate uncertainty empirically via data splitting or repeated resampling
& cross-validation\cite{Arlot_2010_cv}; bootstrap\cite{bootstrap} \\

\bottomrule
\end{tabular}
\end{table}

None of these approaches simultaneously achieve the combination of distribution-free operation, finite-sample validity, and instance-wise guarantees that conformal prediction provides. Nevertheless, each method family has its own advantages and suitable application contexts. The remainder of this section outlines the  several representative competitors and contrasts them with conformal prediction to highlight its distinct value.

\subsection{Bayesian Framework}
Bayesian methods can yield efficient and well-calibrated intervals when the likelihood and prior are correctly specified, and they naturally account for parameter uncertainty through the posterior. However, their guarantees depends on the prior.Under misspecification, Bayesian credible intervals often suffer from severe undercoverage despite appearing narrow, and the Bayesian framework provides no  coverage guarantee.

In contrast, conformal prediction is agnostic to the data-generating mechanism and avoids reliance on priors or likelihood models. Its intervals remain finite-sample valid under the sole assumption of exchangeability, maintaining stable coverage even when the predictive model is misspecified. The trade-off is efficiency: conformal intervals are typically wider, particularly at high confidence levels or with limited calibration data, because they must guard against worst-case residual behavior.

In this section, the comparisons are organized across three settings. 
First, full Bayesian and empirical Bayesian approaches are examined in the context of ridge regression, contrasting full posterior integration with the empirical Bayes approximation that estimates prior hyperparameters from data. 
Second, the analysis is extended to a Bayesian neural network on the UCI ENB2012 Energy dataset\cite{energy_efficiency_242}, using an MLP backbone to isolate the effect of the Bayesian treatment itself. 
Together, these experiments provide a unified view of how Bayesian uncertainty behaves in both linear and nonlinear models, and how it compares with distribution-free conformal prediction.


\subsubsection{Full Bayesian and Empirical Bayesian}
This experiment based on the general setup of \cite{BayesVsTypicalness} and compare the interval performance of Full Bayesian Ridge Regression and Split Conformal Prediction using generated data. The input features \(x \in [-10,10]^5\) are independently sampled from a uniform distribution, and the weight vector \(w\) is drawn from a standard normal prior \( \mathcal{N}(0, I) \). Labels are generated according to
\[
y = x^\top w + \varepsilon, \qquad \varepsilon \sim \mathcal{N}(0,1).
\]
A single training–test split (100 training points and 100 test points) is fixed using a predetermined random seed, and we evaluate 20 significance levels \(\alpha \in [0.01,0.99]\) on this fixed dataset. Each \((a,\alpha)\) configuration is repeated 10 times: Bayesian Ridge uses the same dataset in all repetitions, whereas Split Conformal varies only the internal random seed that determines the calibration split.

Bayesian Ridge Regression applies
\[
\Sigma = (X^\top X + aI)^{-1}, \qquad \mu = \Sigma X^\top y,
\]
and constructs Gaussian predictive intervals via
$$
y \mid \mathcal{D} \sim \mathcal{N}\!\left(x\mu,\; 1 + x\Sigma x\right),
$$
evaluating the effect of prior misspecification with \(a \in \{1, 1000, 10000\}\).

Split Conformal Prediction uses the same Ridge predictor but randomly partitions the training data into \(80\%\) proper training and \(20\%\) calibration subsets, forming distribution-free intervals from the finite-sample quantile of the calibration residuals.

Across all confidence $(1-\alpha)$, this experimentevaluate both the empirical coverage and the mean interval width, thereby comparing the sensitivity of Bayesian intervals to prior misspecification against the distribution-free robustness of Split Conformal Prediction.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figure/FullBayes_vs_SplitCP.png}
    \caption{Full Bayesian RR and Split Conformal prediction for RR.}
    \label{fig:fullbayes_vs_splitcp}
\end{figure}

In addition, since Full Bayesian methods are no longer the preferred choice after the introduction of Empirical Bayes, the experiment also incorporates an Empirical Bayesian variant to provide a more modern and practically relevant comparison.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figure/EmpiricalBayes_vs_SplitCP.png}
    \caption{Empirical Bayesian RR and Split Conformal prediction for RR.}
    \label{fig:empbayes_vs_splitcp}
\end{figure}



As shown in figure ~\ref{fig:fullbayes_vs_splitcp}, when the prior is correctly specified (\(a = 1\)), Bayesian Ridge Regression produces valid prediction intervals. As \(a\) increases and the prior becomes misspecified, the Bayesian intervals deteriorate sharply: their coverage collapses while the interval width remains almost unchanged. In contrast, the width of the Split Conformal intervals increases with the confidence level and maintains valid coverage across all settings.

A similar pattern appears in the empirical Bayesian variant in ~\ref{fig:empbayes_vs_splitcp}. With the correct prior, it achieves good coverage with intervals narrower than those of the full Bayesian method. Under severe prior misspecification, the empirical Bayesian intervals widen and display some degree of robustness, yet their coverage still remains below that of Split Conformal Prediction, can not reach the desired level.



%介绍数据集
\subsubsection{Bayesian Neural Network}
Recent years have introduced improved Bayesian neural network (BNN) methods \cite{BNN}. 
Their core idea is simple: instead of learning a single set of network weights, a BNN places a prior over the weights and learns a posterior distribution. 
Sampling from this posterior gives multiple plausible models, allowing the prediction uncertainty to reflect both model uncertainty (epistemic) and data noise (aleatoric). 

This experiment applies such a BNN to the UCI ENB2012 Energy dataset\cite{energy_efficiency_242} , using the same MLP backbone as in the CP baseline. 

The UCI ENB2012 Energy dataset contains 768 simulated building designs, each described by eight physical and geometric features such as wall area, roof area, glazing ratio, and overall height. These features are used to predict two real-valued heating and cooling load indicators. The dataset is low-dimensional, noise-modest, and exhibits smooth nonlinear structure, making it a standard benchmark for regression and uncertainty quantification. 

In this experiment, both the BNN and Split CP use the same base learner: an MLP with input dimension
$8$, one hidden layer of width $64$ with ReLU. 
The BNN adopts a mean–field variational formulation
(Bayes-by-Backprop style)\cite{blundell2015weight}, placing independent Gaussian priors on all weights and learning Gaussian
posterior parameters via an ELBO objective with reparameterized sampling.


and a $1$--dimensional output. The BNN employs Gaussian priors $w,b\sim\mathcal N(0,1)$ and is
trained for $100$ epochs using Adam ($10^{-3}$ learning rate, batch size $64$). Posterior prediction
is approximated with $T=200$ Monte Carlo samples, yielding predictive mean $\mu$ and standard
deviation $\sigma$ (after inverting the response scaling). The
$(1-\alpha)$ interval is
$
\mu \pm z_{1-\alpha/2}\,\sigma.
$

For Split CP, the same MLP is trained on $70\%$ of the data, with the remaining $30\%$ used for
calibration. Absolute residuals are sorted, and the conformal radius for level $\alpha$ is taken at
index $\lceil (n_{\text{cal}}+1)(1-\alpha)\rceil$. CP intervals are obtained by adding/subtracting
this radius to the MLP predictions on the raw scale. 





\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figure/BNN_vs_SplitCP.png}
    \caption{BNN and Split CP on UCI ENB2012.}
    \label{fig:bnn_vs_splitcp}
\end{figure}

Figure~\ref{fig:bnn_vs_splitcp} shows that, under the same MLP base model, BNN intervals deviate sharply from the ideal diagonal: the fraction of points outside the intervals is far above nominal, reflecting severely underestimated posterior variance and strong overconfidence. The intervals remain narrow across confidence levels, consistent with the observed undercoverage.

By contrast, Split Conformal closely follows the ideal line, with interval width increasing
predictably with confidence. Although wider, these intervals achieve near-nominal coverage.

\subsection{Boostrap Resampling}
Bootstrap is a non-parametric resampling method used to assess the variability, bias, and confidence intervals of statistical estimators. It treats the observed data as an empirical distribution and repeatedly draws samples with replacement to mimic how the estimator would behave if the experiment were replicated. This mechanism does not rely on explicit distributional assumptions, making it conceptually similar to conformal prediction in its distribution-free spirit.

However, bootstrap also has notable drawbacks. Classical bootstrap requires retraining the model and re-evaluating predictions for every resampled dataset; achieving stable estimates typically demands a large number of resamples, leading to significant computational cost. 

The experiment evaluates the finite-sample behavior of Bootstrap prediction intervals and Split CP using the ENB2012 Heating Load dataset. The first eight features are used as inputs, and the heating load target is standardized for training stability. The dataset is randomly split into training (80\%) and test (20\%). 

A common base learner is adopted for both methods: a two-layer MLP (64 hidden units, ReLU activations) trained for 200 epochs with Adam. Split CP trains a single model and uses calibration residuals to obtain a valid finite-sample quantile. In contrast, Bootstrap requires training (B) independent models on resampled datasets and aggregates their test predictions to form empirical $((\alpha/2, 1-\alpha/2))$ quantiles.

The experiment vary $(B \in {5,10,20,100,300,600})$ to examine how computation time increases and how coverage behaves across resampling budgets. For each method, record empirical coverage, average interval length (after inverse scaling), and wall-clock computation time.

Table \ref{boostrapSlow} reports the Bootstrap prediction interval results and the corresponding total computation times. Split Conformal Prediction, by contrast, requires only a single model fit and achieves a total runtime of approximately 0.68 s with empirical coverage 0.89. This setup isolates the computational burden of Bootstrap and assesses whether increasing B improves its coverage.

\begin{table}[t]
\centering
\begin{tabular}{llll}
\toprule
$B$ & Coverage & Avg.\ length & Total time (s) \\
\midrule
5   & 0.448 & 2.09 & 3.00 \\
10  & 0.604 & 2.37 & 6.06 \\
20  & 0.604 & 2.79 & 12.03 \\
100 & 0.688 & 3.10 & 61.36 \\
300 & 0.747 & 3.23 & 181.74 \\
600 & 0.753 & 3.23 & 359.54 \\
\bottomrule
\end{tabular}
\caption{Bootstrap : larger $B$ increases time but still lacks coverage.}
\label{boostrapSlow}
\end{table}

Bootstrap is not categorically inferior to conformal prediction. When the computational cost of increasing the resampling budget \(B\) is acceptable, bootstrap can produce smaller prediction sets, yielding more efficient predictions.

The experiment uses a misspecified regression setting. Covariates 
$X \in [-1,1]^d$ are sampled i.i.d.\ from a uniform distribution, and the response is generated as
\[
Y = X^\top \beta \;+\; 2X_1^2 \;+\; X_2^2 \;+\; \varepsilon,\qquad 
\varepsilon \sim \mathcal{N}(0,\sigma^2),
\]
so the true model is nonlinear while a linear model is fitted.

Split conformal prediction trains a linear regressor on $90\%$ of the training data and uses the remaining $10\%$ for calibration.  
The bootstrap method fits a linear model on the full training set and constructs predictive intervals by residual resampling with $B$ bootstrap replicates.

\begin{table}[t]
\centering
\begin{tabular}{lll}
\toprule
Method & Coverage & Avg.\ Length \\
\midrule
Split Conformal & $0.927 \pm 0.063$ & $4.825 \pm 1.145$ \\
Bootstrap       & $0.884 \pm 0.342$ & $3.911 \pm 0.342$ \\
\bottomrule
\end{tabular}
\caption{Coverage and efficiency under model misspecification.}
\label{BoostrapEfficiency}
\end{table}

Table~\ref{BoostrapEfficiency} illustrates this trade-off. This aligns with the view that bootstrap is not categorically inferior to conformal prediction; when the computational cost of a sufficiently large resampling budget $B$ is acceptable, bootstrap can produce smaller and more efficient prediction sets.bootstrap shows greater variability but yields consistently shorter intervals.


\subsection{Quantile Regression}

Quantile regression directly learns conditional quantile functions by minimizing asymmetrically weighted absolute deviations, capturing systematic variation across quantile levels \cite{koenker2001quantile}. This allows it to reveal heteroscedasticity, tail behavior, and distributional asymmetry, making it highly attractive in many economic and financial applications.

The previous experimental setup is retained, using the ENB2012 Heating Load dataset with the first eight features as predictors. Under this setting and with a shared MLP base learner, Split CP attains coverage 0.896 with an average length of 2.35, close to the nominal 90\% level. Quantile regression, despite using the same architecture, yields lower coverage 0.799 and wider intervals 3.59.

A key limitation relative to CP is that the quantile level $\tau$ is coupled with model training. Specifying different $\tau$ values (for example, $0.05$, $0.5$, $0.95$) requires training a separate model for each. To obtain an interval
$
[q_{\alpha/2}(x),\, q_{1-\alpha/2}(x)]
$,
two quantile regression models must be fitted. This is inherent to the quantile-regression objective, as the loss function depends explicitly on $\tau$, so each $\tau$ yields a different optimizer. In contrast, conformal prediction is more flexible: it does not require retraining the base model when the desired coverage level changes.

More importantly, quantile regression and conformal prediction are substantially complementary.  
Quantile regression directly fits the conditional lower and upper quantiles and therefore automatically captures conditional heteroscedasticity (interval width varying with $x$) and local density variation reflected in quantile spacing.  

By contrast, CP—unless a specially designed score is used to encode local variability (e.g., Adaptive Prediction Sets \cite{angelopoulos2020uncertainty}) learns only marginal information from residuals. Its validity guarantee is marginal, and the resulting intervals may lose efficiency on data with strong heteroscedasticity or asymmetric conditional distributions.  

This motivates recent methods that combine the two approaches: quantile regression provides a flexible estimate of the conditional distribution, while conformalization adds finite-sample marginal validity, yielding interval estimators that are both adaptive and distribution-free.


Sesia and Candès \cite{sesia2020CQR_Comparing} present a systematic comparison of methods that integrate conformal prediction with quantile regression. Their study examines three variants—CQR, CQR-m, and CQR-r—distinguished by their constructions of conformity scores, and establishes that all variants are asymptotically efficient, with prediction bands converging to the oracle conditional quantile interval under mild conditions.

Through experiments on synthetic and real datasets, the authors show that these procedures maintain finite-sample validity, with CQR typically producing the narrowest intervals. They also provide practical recommendations for data splitting, indicating that allocating approximately 70\%–90\% of the sample to quantile-regression training yields a favorable balance between adaptivity and stability.

\section{Implications of a World Without Conformal Prediction}

As discussed in Section 4, the core contribution of CP lies in its decoupling principle, whose conceptual influence extends beyond a single algorithm. Without conformal prediction, the field would also lose a clear illustration of this modular design idea, which is potentially useful not only for UQ but for other areas of machine learning as well.


This monograph by Balasubramanian, Ho and Vovk\cite{Vovk2014conformal} surveys a large number of applications, most of which crucially exploit that conformal prediction provides provably valid and well-calibrated prediction regions under nothing more than an exchangeability assumption, outputs set-valued predictions with user-specified confidence levels whose empirical error frequencies match the nominal significance, and can be used as a model-agnostic wrapper around virtually any underlying classifier or regressor (including SVMs, neural networks, k-NN, ridge regression, etc.). 

A particularly insightful observation, originally articulated by Vovk in an invited talk, concerns the potential role of conformal prediction in grounding large language models. The key argument is that contemporary LLMs lack an external notion of truth: much of what they produce reflects statistical regularities of language rather than factual grounding. From this viewpoint, conformal prediction provides a principled mechanism for injecting verifiable ground-truth signals into models whose training is otherwise purely text-driven.

This perspective has already inspired concrete methodological developments. For example, Cherian et al.~\cite{cherian2024large} propose enhanced conformal procedures for evaluating and calibrating LLM outputs, illustrating how CP can furnish validity guarantees even in settings where conventional ground truth is scarce or difficult to access.

Building on this perspective, the experiments in Section 5 show that, in the considered benchmarks, none of the competing methods (Bayesian credible intervals, bootstrap, or quantile regression) simultaneously attains such distribution-free finite-sample coverage and algorithm-agnostic deployability, so they cannot be regarded as full substitutes for conformal prediction. 

Without conformal prediction, the UQ landscape would lack a universally applicable and theoretically rigorous framework; applying alternative methods would require researchers and practitioners to verify and tune model-specific assumptions and hyperparameters on a case-by-case basis, often without any distribution-free finite-sample guarantees. This would raise the technical barrier for deploying reliable UQ across domains and could slow the adoption of trustworthy machine-learning systems, while increasing the risk of calibration failures in real applications.



\section{Conclusion}
Conformal prediction offers more than a set of algorithms; it provides a structural template for uncertainty quantification that separates model construction from validity calibration. This decoupling principle enables a wide range of predictors to be equipped with distribution-free, finite-sample guarantees through a unified mechanism based on score ranking under exchangeability.

The comparative analysis in this paper shows that competing approaches each address only part of this objective. Bayesian methods provide efficient intervals when correctly specified but lack robustness under misspecification. Bootstrap resampling is flexible but computationally demanding and unstable in small samples. Quantile regression captures conditional structure but does not inherently guarantee empirical coverage. None of these methods simultaneously deliver model-agnostic deployment, finite-sample validity, and freedom from distributional assumptions.

In a hypothetical landscape without conformal prediction, practitioners would have to rely more heavily on methods tied to model-specific assumptions and case-by-case diagnostics, without access to a general framework offering distribution-free validity guarantees. The lack of such a unifying principle could make it harder to develop and analyze reliable machine-learning procedures.

From this perspective, conformal prediction can be viewed not only as a practically useful tool, but also as a methodological framework that has provided a coherent way to articulate and extend ideas about validity and uncertainty beyond its original formulation.


\small
\bibliographystyle{unsrtnat}   % 按出现顺序排序
\bibliography{reference}     % 对应 references.bib 文件


\end{document}