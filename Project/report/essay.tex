\documentclass{article}

\usepackage[final]{neurips_2019}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{float}   % 放在导言区
\usepackage{array}

\usepackage{graphicx}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{float}  % 需要这个包
\usepackage{amsmath}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}

\title{What If Without the Conformal Prediction?}

\author{
  Jing LIANG\\
  Industrial Engineering and Decision Analytics\\
  Hong Kong University of Science and Technology\\
  \texttt{jliangcd@connect.ust.hk} \\
}

\begin{document}

\maketitle

\begin{abstract}
  To be continued...
  % The abstract paragraph should be indented \nicefrac{1}{2}~inch (3~picas) on
  % both the left- and right-hand margins. Use 10~point type, with a vertical
  % spacing (leading) of 11~points.  The word \textbf{Abstract} must be centered,
  % bold, and in point size 12. Two line spaces precede the abstract. The abstract
  % must be limited to one paragraph.
\end{abstract}

\section{Introduction}

With the growing use of machine learning in risk-sensitive domains such as medical diagnosis and financial risk management, aggregate metrics such as average accuracy are no longer sufficient. Conventional learning algorithms such as neural networks, support vector machines, and decision trees typically provide only point predictions and do not offer calibrated measures of uncertainty. In practice, decision-makers require not only accurate predictions but also a principled assessment of their reliability.

Conformal prediction addresses this need by constructing, for each new input $X_{n+1}$, a prediction set
\[
  C_\alpha(X_{n+1}) \subseteq \mathcal{Y},
\]
where $\mathcal{Y}$ denotes the label space and $\alpha \in (0,1)$ is the target miscoverage level. The goal is to ensure the finite-sample, distribution-free validity guarantee
\[
  \mathbb{P}\!\left( Y_{n+1} \in C_\alpha(X_{n+1}) \right) \ge 1 - \alpha ,
\]
for the unknown true label $Y_{n+1}$, assuming only that the data $(X_i,Y_i)_{i=1}^{n+1}$ are exchangeable.

To achieve this, conformal prediction defines a \emph{nonconformity score}
\[
  V_i = V(X_i, Y_i), \qquad i = 1,\dots,n,
\]
which measures how atypical an example $(X_i,Y_i)$ is relative to the others. A candidate label $y$ for the new input $X_{n+1}$ is evaluated via its test-time score $V(X_{n+1}, y)$ and included in the prediction set whenever it does not exceed the appropriate empirical quantile of the calibration scores.

If conformal prediction did not exist, what we would lose is not merely an algorithm but a critical meta-methodology: a systematic framework that decouples uncertainty quantification from model design through the construction of nonconformity scores. The impact would extend beyond the disappearance of a single technique, altering the fundamental paradigm for developing uncertainty-aware predictive systems.


% This paper examines this question from multiple perspectives, including the historical development of conformal prediction, its core insights, and its distinctive role in modern machine learning. Sections 2 and 3 provide an in-depth overview of its evolution and underlying principles, clarifying its unique value and highlighting the essential differences from alternative approaches to uncertainty quantification. Section 4 presents a comparative analysis between conformal prediction and other uncertainty quantification methods, demonstrating its advantages and irreplaceability. Section 5 discusses its practical significance in real-world applications and explores the potential consequences for research and practice in a hypothetical scenario where conformal prediction does not exist, including the research directions that would be absent in its absence.
%这部分要根据文章结构调整一下


\section{Background and Need}



With the rapid development of artificial intelligence and its widespread deployment across society, both industry and government agencies are increasingly adopting AI-based decision-support systems and agentic task-execution systems. These systems are typically built upon predictive machine learning models, which generate predictions or recommended actions through supervised learning tasks\cite{SocialImpactAI}.

At the same time, AI technologies are accelerating their penetration into high-risk domains such as medical diagnosis, biometrics, face recognition, nuclear fusion, and industrial diagnostics\cite{Vovk2014conformal}. In these settings, model failures can lead to severe medical, ethical, economic, or safety consequences. As a result, society and regulatory bodies have raised expectations regarding the reliability, fairness, transparency, and accountability of AI systems, thereby driving the development of Trustworthy AI (TAI).

Within the TAI framework, uncertainty quantification (UQ) plays a central role in ensuring reliability. The goal of UQ is to characterize and evaluate various sources of uncertainty in predictive models, providing reliable and calibrated confidence information\cite{smith2024uncertainty}. This enables decision-makers to understand the trust boundaries of model outputs and make robust decisions accordingly.

Traditional UQ approaches face structural limitations that constrain their effectiveness in high-risk applications. Bayesian methods, such as Bayesian neural networks, provide probabilistic predictions but rely heavily on strong prior assumptions and the correctness of the model specification; inference is also computationally expensive for large-scale models. Frequentist methods, such as bootstrapping or dropout-based UQ, estimate uncertainty through resampling or approximate inference but typically lack distribution-free, finite-sample guarantees. They also become unstable under distribution shift and incur high computational costs.

Consequently, traditional UQ methods generally fail to provide instance-wise, distribution-free guarantees of validity.

Conformal prediction, however, offers a systematic solution. It provides verifiable, finite-sample validity guarantees for each individual instance without requiring any distributional assumptions, enabling precise quantification of predictive uncertainty.


\section{Conformal Prediction}
\subsection{Historical Development}

The theoretical origins of conformal prediction were established in the late 1990s and early 2000s by Vladimir Vovk, Alex Gammerman, and Glenn Shafer\cite{OriginTrasnduction,ICM,vovk2005conformal}. Unlike traditional parametric statistics or Bayesian inference, the methodology arises from the frameworks of algorithmic randomness and game-theoretic probability\cite{vovk2005conformal,Gammerman1999Kolmogorov}. Its objective is to construct distribution-free predictive guarantees for machine learning that remain valid for finite samples. Within this paradigm, predictions produce sets of plausible labels rather than single point estimates, and the size of these sets adapts to model uncertainty, thereby providing a direct quantification of predictive reliability.


The foundational contribution to the field is the work that establishes the theoretical basis of conformal prediction \citep{vovk2005conformal}. A more recent overview is provided by the monograph “Conformal prediction for reliable machine learning: theory, adaptations and applications” \citep{Vovk2014conformal}.

Recent developments in conformal prediction have focused on three main areas: improving the validity concept, enhancing computational efficiency, and expanding application scope\citep{fontana2023conformal}.

% To achieve a stronger guarantee than basic marginal coverage, methods like Mondrian CP (MCPs) were introduced to ensure conditional validity within defined data categories, and Conformalized Quantile Regression (CQR) was developed to generate adaptive prediction intervals for heteroscedastic data. In terms of efficiency, the Inductive CP (ICP) framework has become dominant . ICP splits data into training and calibration sets, dramatically lowering the computation cost compared to the original transductive CP. More advanced versions, such as Jackknife+ and Cross-conformal prediction, further enhance stability and data utilization. Finally, CP's applicability has been extended to complex scenarios, including Functional Data Analysis (FDA) for function prediction bands, as well as developing robust extensions for non-i.i.d. data like time series and situations involving covariate shift.
%先注释掉，如果后面的内容感觉不太够的话这里再凑一点工作量

\subsection{Conformal Prediction Framework}
Vovk’s work introduces full conformal prediction or transductive conformal prediction \cite{vovk2005conformal}. The central idea is that, given a training set and a new test instance, each possible label for the test instance is tentatively paired with the training data to compute a nonconformity score, and the label is included in the prediction set based on its rank among these scores. Because this procedure becomes computationally expensive as the dataset grows, subsequent research has proposed more efficient variants, most notably split conformal prediction\cite{SplitConformalPrediction}, which is now widely used. This subsection will focus on split conformal prediction and its technical details, and illustrate the overall conformal framework.

\subsubsection{Split Conformal Prediction}

Recall the targert of conformal prediction : given i.i.d.\ data $(X_i,Y_i)_{i=1}^n$ with $X_i\in\mathcal{X}$, $Y_i\in\mathcal{Y}$ and a target
miscoverage level $\alpha\in(0,1)$, conformal prediction attempts to construct, for
each new input $X_{n+1}$, a prediction set
\[
  C_{\alpha}(X_{n+1}) \subseteq \mathcal{Y}
\]
such that the coverage rate guarantee
\[
  \mathbb{P}\!\left( Y_{n+1} \in C_{\alpha}(X_{n+1}) \right) \ge 1-\alpha
\]
holds for any underlying distribution, assuming only exchangeability (i.i.d.) of
$(X_i,Y_i)_{i=1}^{n+1}$.


Split conformal prediction achieves this by partitioning the data into two parts: a
training subset for fitting an arbitrary base model $\hat f$, and a calibration subset for
quantifying the model’s residual uncertainty.  
The calibration samples provide an empirical distribution of nonconformity scores, whose
appropriate quantile is then used to expand the model’s raw prediction into a prediction
set with controlled miscoverage.  
In this way, split CP (Conformal Prediction) converts any point predictor into a distribution-free, finite-sample
valid prediction region.



\begin{algorithm}[H]
\caption{Split Conformal Prediction }
\label{alg:split-conformal}
\begin{algorithmic}[1]
\REQUIRE Dataset $\{(X_i, Y_i)\}_{i=1}^n$; base learner $\mathcal{A}$; nonconformity function $V_f$; miscoverage level $\alpha \in (0,1)$
\ENSURE Mapping $x \mapsto C_\alpha(x)$

\STATE Randomly split the index set $\{1,\dots,n\}$ into disjoint subsets:
       training indices $I_{\mathrm{train}}$ and calibration indices $I_{\mathrm{cal}}$

\STATE Fit the prediction model on the training set:
       \[
         f \leftarrow \mathcal{A}\bigl(\{(X_i, Y_i) : i \in I_{\mathrm{train}}\}\bigr)
       \]

\STATE For each $i \in I_{\mathrm{cal}}$, compute the nonconformity score:
       \[
         V_i \leftarrow V_f(X_i, Y_i), \qquad i \in I_{\mathrm{cal}}
       \]

\STATE Let $m \leftarrow |I_{\mathrm{cal}}|$ and
       $k \leftarrow \bigl\lceil (1-\alpha)(m+1)\bigr\rceil$.
       Define $\widehat{Q}_{1-\alpha}$ as the $k$-th smallest value in
       $\{V_i : i \in I_{\mathrm{cal}}\}$

\STATE For any new input $x \in \mathcal{X}$, define the prediction set:
       \[
         C_\alpha(x)
           \leftarrow \bigl\{y \in \mathcal{Y} : V_f(x,y) \le \widehat{Q}_{1-\alpha}\bigr\}
       \]

\end{algorithmic}
\end{algorithm}
Moreover, the split conformal prediction algorithm comes with a rigorous theoretical coverage guarantee.

\begin{theorem}[Conformal coverage guarantee for Split CP\cite{angelopoulos2021gentle}]
Suppose the observations $(X_i,Y_i)_{i=1,\ldots,n}$ and the test pair
$(X_{\mathrm{test}},Y_{\mathrm{test}})$ are i.i.d.\ and let
$\widehat{Q}_{1-\alpha}$ and $C_\alpha(X_{\mathrm{test}})$ be defined as in
Algorithm~\ref{alg:split-conformal}. Then the following coverage guarantee holds:
\[
  \mathbb{P}\!\left( Y_{\mathrm{test}} \in C_\alpha(X_{\mathrm{test}}) \right)
  \;\ge\; 1-\alpha.
\]
\end{theorem}

Notice that this theorem shows that the coverage guarantee of split CP is not asymptotic but holds non-asymptotically for any sample size.

\subsubsection{Variancs and Their Shared Framework}
In recent years, conformal prediction has attracted considerable attention and has advanced rapidly. In the following, we provide several representative variants and explain how the components that vary, as well as those that remain invariant, help reveal the essential structure of the conformal prediction framework.


\begin{table}[H]
\centering
\begin{tabular}{
    >{\raggedright\arraybackslash}p{2.5cm}
    >{\raggedright\arraybackslash}p{3.5cm}
    >{\raggedright\arraybackslash}p{7cm}
}
\toprule
\textbf{Method} & \textbf{Objective} & \textbf{Core Idea} \\
\midrule

CV+ / Jackknife+ \cite{jackknifePlus}
& Improve statistical efficiency and avoid overly wide intervals from data splitting 
& Use cross-validation or leave-one-out residuals to obtain more stable error estimates and produce tighter prediction sets. \\

\addlinespace

Weighted CP \cite{weightedConformal}
& Achieve valid coverage under covariate shift or population heterogeneity 
& Rank scores using importance weights that reflect differences between the target and calibration distributions. \\

\addlinespace

Distributional CP \cite{distributionalConformal}
& Handle complex, multimodal, or asymmetric conditional distributions 
& Construct scores based on estimated CDF values (e.g., $|F(x,y)-0.5|$) to form high-density prediction regions. \\


\addlinespace

CP for hierarchical structure data \cite{hierarchicalConformal}
& Handle data with group-level heterogeneity and hierarchical sampling structures
& Replace standard exchangeability with hierarchical exchangeability and construct prediction sets using pooled CDF, double conformal, or subsampling-based aggregation across groups. \\

\addlinespace

Adaptive Prediction Sets\cite{romano2020classification,angelopoulos2020uncertainty}
& Improve fairness of coverage across classes and sample difficulty 
& Use cumulative softmax probability to define an ordered score and adjust prediction set size according to sample difficulty. \\

\addlinespace

Conformalized Quantile Regression\cite{CQR}
& Construct more adaptive regression intervals 
& Measure deviation from upper and lower quantiles and calibrate these to form intervals adaptive to conditional distribution shape. \\


\bottomrule
\end{tabular}
\end{table}


From the table, we can see that although recent developments in conformal prediction have produced many variants, their innovations mainly focus on how the score is designed, what is calibrated, or how the exchangeability assumption is extended.
These represent the variant component of the CP framework. Different methods modify the source of the scores, the geometric form of the scores, or model the underlying data structure and distribution shift to better adapt to new tasks, data structures, or distributional settings. These changes improve the applicability of CP in complex, high-dimensional, heteroscedastic, multimodal, and non-i.i.d. scenarios, allowing the framework to remain flexible across different use cases.


In contrast, all methods preserve the invariant component of conformal prediction. No matter how the method varies, the validity mechanism follows the same basic design: construct a score that measures the discrepancy between the prediction and the ground truth, compute its empirical distribution on the calibration set, and rely on exchangeability to obtain validity guarantees.

\section{Key Insights and Benefit of Conformal Prediction}
% 从section3.2.2中我们可以知道conformal prediction有很多的变种，但是不管是哪一种变种，他们都遵循着conformal prediction的基本框架。



Therefore, the true insight of conformal prediction is that it offers a meta-methodology:

1. Decoupling: It separates model performance (implicitly reflected through the score function A) from output guarantees (derived from rank-based calibration and exchangeability).

2. Guarantee-first design: It reverses the traditional approach. Instead of trying to make model outputs “look like” probabilities and then hoping for guarantees, CP first builds a framework that inherently provides guarantees (via calibration and set-valued prediction), and then embeds any model inside it.


% ovk 在1980–1990年代深入研究了算法随机性的数学基础，提出了“超鞅”（supermartingales）的概念来形式化检测序列中的非随机性或非交换性。他证明，在一个公平的博弈中，如果数据序列存在某种模式或偏差，一个精明的赌徒可以通过构造特定的超鞅策略使其资本无界增长。这一思想后来被直接应用于共形预测中：当一个新的测试样本到来时，通过将其与训练集合并并计算每个可能的标签所对应的“非共形测度”（nonconformity measure），可以构造出一系列p值。这些p值本质上反映了该样本在当前假设下有多“异常”或“不共形”，其计算过程正是基于数据排列的对称性，从而天然地依赖于交换性假设而非i.i.d.

\section{Alternative Methods}

A wide range of uncertainty quantification techniques have been developed across statistics and machine learning. These approaches differ in their underlying assumptions, the form of guarantees they provide, and their suitability for various application settings. Table summarizes the main methodological families commonly used as competitors to conformal prediction.

\begin{table}[H]
\centering
\begin{tabular}{
    >{\raggedright\arraybackslash}p{3.5cm}
    >{\raggedright\arraybackslash}p{4cm}
    >{\raggedright\arraybackslash}p{5cm}
}
\toprule
\textbf{Method Family} & \textbf{Key Idea} & \textbf{Representative Competitors} \\
\midrule

\textbf{Classical parametric prediction intervals} 
& Assume a fully specified parametric model, typically linear with Gaussian noise
& Gaussian linear model (t-interval) \\

\addlinespace

\textbf{Semi- / Non-parametric prediction bands} 
& Assume a functional form for estimation, such as conditional means or quantiles, without making distributional assumptions
& Quantile regression bands \\

\addlinespace

\textbf{Bayesian framework} 
& Characterize uncertainty through the posterior predictive distribution
& Bayesian neural networks; Bayesian model averaging \\

\addlinespace

\textbf{PAC-learning} 
& Provide worst-case upper bounds on prediction error over the hypothesis class
& Vapnik–Chervonenkis bounds; Littlestone–Warmuth bounds \\

\addlinespace

\textbf{Hold-out and resampling-based methods} 
& Estimate uncertainty empirically via data splitting or repeated resampling
& Train–test hold-out; cross-validation; bootstrap \\

\bottomrule
\end{tabular}
\end{table}

None of these approaches simultaneously achieve the combination of distribution-free operation, finite-sample validity, and instance-wise guarantees that conformal prediction provides. Nevertheless, each method family has its own advantages and suitable application contexts. The remainder of this section outlines the core ideas behind several representative competitors and contrasts them with conformal prediction to highlight its distinct value.

\subsection{Bayesian Framework}
Bayesian methods can yield efficient and well-calibrated intervals when the likelihood and prior are correctly specified, and they naturally account for parameter uncertainty through the posterior. However, their guarantees depends on the prior.Under misspecification, Bayesian credible intervals often suffer from severe undercoverage despite appearing narrow, and the Bayesian framework provides no  coverage guarantee.

In contrast, conformal prediction is agnostic to the data-generating mechanism and avoids reliance on priors or likelihood models. Its intervals remain finite-sample valid under the sole assumption of exchangeability, maintaining stable coverage even when the predictive model is misspecified. The trade-off is efficiency: conformal intervals are typically wider, particularly at high confidence levels or with limited calibration data, because they must guard against worst-case residual behavior.

In this section, the comparisons are organized across three settings. 
First, full Bayesian and empirical Bayesian approaches are examined in the context of ridge regression, contrasting full posterior integration with the empirical Bayes approximation that estimates prior hyperparameters from data. 
Second, the analysis is extended to a Bayesian neural network on the UCI ENB2012 Energy dataset\cite{energy_efficiency_242}, using an MLP backbone to isolate the effect of the Bayesian treatment itself. 
Together, these experiments provide a unified view of how Bayesian uncertainty behaves in both linear and nonlinear models, and how it compares with distribution-free conformal prediction.


\subsubsection{Full Bayesian and Empirical Bayesian}
This experiment based on the general setup of \cite{BayesVsTypicalness} and compare the interval performance of Full Bayesian Ridge Regression and Split Conformal Prediction using generated data. The input features \(x \in [-10,10]^5\) are independently sampled from a uniform distribution, and the weight vector \(w\) is drawn from a standard normal prior \( \mathcal{N}(0, I) \). Labels are generated according to
\[
y = x^\top w + \varepsilon, \qquad \varepsilon \sim \mathcal{N}(0,1).
\]
A single training–test split (100 training points and 100 test points) is fixed using a predetermined random seed, and we evaluate 20 significance levels \(\alpha \in [0.01,0.99]\) on this fixed dataset. Each \((a,\alpha)\) configuration is repeated 10 times: Bayesian Ridge uses the same dataset in all repetitions, whereas Split Conformal varies only the internal random seed that determines the calibration split.

Bayesian Ridge Regression applies
\[
\Sigma = (X^\top X + aI)^{-1}, \qquad \mu = \Sigma X^\top y,
\]
and constructs Gaussian predictive intervals via
$$
y \mid \mathcal{D} \sim \mathcal{N}\!\left(x\mu,\; 1 + x\Sigma x\right),
$$
evaluating the effect of prior misspecification with \(a \in \{1, 1000, 10000\}\).

Split Conformal Prediction uses the same Ridge predictor but randomly partitions the training data into \(80\%\) proper training and \(20\%\) calibration subsets, forming distribution-free intervals from the finite-sample quantile of the calibration residuals.

Across all confidence $(1-\alpha)$, this experimentevaluate both the empirical coverage and the mean interval width, thereby comparing the sensitivity of Bayesian intervals to prior misspecification against the distribution-free robustness of Split Conformal Prediction.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figure/FullBayes_vs_SplitCP.png}
    \caption{Full Bayesian RR and Split Conformal prediction for RR.}
    \label{fig:fullbayes_vs_splitcp}
\end{figure}

In addition, since Full Bayesian methods are no longer the preferred choice after the introduction of Empirical Bayes, the experiment also incorporates an Empirical Bayesian variant to provide a more modern and practically relevant comparison.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figure/EmpiricalBayes_vs_SplitCP.png}
    \caption{Empirical Bayesian RR and Split Conformal prediction for RR.}
    \label{fig:empbayes_vs_splitcp}
\end{figure}



As shown in figure ~\ref{fig:fullbayes_vs_splitcp}, when the prior is correctly specified (\(a = 1\)), Bayesian Ridge Regression produces valid prediction intervals. As \(a\) increases and the prior becomes misspecified, the Bayesian intervals deteriorate sharply: their coverage collapses while the interval width remains almost unchanged. In contrast, the width of the Split Conformal intervals increases with the confidence level and maintains valid coverage across all settings.

A similar pattern appears in the empirical Bayesian variant in ~\ref{fig:empbayes_vs_splitcp}. With the correct prior, it achieves good coverage with intervals narrower than those of the full Bayesian method. Under severe prior misspecification, the empirical Bayesian intervals widen and display some degree of robustness, yet their coverage still remains below that of Split Conformal Prediction, can not reach the desired level.



%介绍数据集
\subsubsection{Bayesian Neural Network}
Recent years have introduced improved Bayesian neural network (BNN) methods \cite{BNN}. 
Their core idea is simple: instead of learning a single set of network weights, a BNN places a prior over the weights and learns a posterior distribution. 
Sampling from this posterior gives multiple plausible models, allowing the prediction uncertainty to reflect both model uncertainty (epistemic) and data noise (aleatoric). 

This experiment applies such a BNN to the UCI ENB2012 Energy dataset\cite{energy_efficiency_242} , using the same MLP backbone as in the CP baseline. 

The UCI ENB2012 Energy dataset contains 768 simulated building designs, each described by eight physical and geometric features such as wall area, roof area, glazing ratio, and overall height. These features are used to predict two real-valued heating and cooling load indicators. The dataset is low-dimensional, noise-modest, and exhibits smooth nonlinear structure, making it a standard benchmark for regression and uncertainty quantification. 

In this experiment, both the BNN and Split CP use the same base learner: an MLP with input dimension
$8$, one hidden layer of width $64$ with ReLU. 
The BNN adopts a mean–field variational formulation
(Bayes-by-Backprop style)\cite{blundell2015weight}, placing independent Gaussian priors on all weights and learning Gaussian
posterior parameters via an ELBO objective with reparameterized sampling.


and a $1$--dimensional output. The BNN employs Gaussian priors $w,b\sim\mathcal N(0,1)$ and is
trained for $100$ epochs using Adam ($10^{-3}$ learning rate, batch size $64$). Posterior prediction
is approximated with $T=200$ Monte Carlo samples, yielding predictive mean $\mu$ and standard
deviation $\sigma$ (after inverting the response scaling). The
$(1-\alpha)$ interval is
$
\mu \pm z_{1-\alpha/2}\,\sigma.
$

For Split CP, the same MLP is trained on $70\%$ of the data, with the remaining $30\%$ used for
calibration. Absolute residuals are sorted, and the conformal radius for level $\alpha$ is taken at
index $\lceil (n_{\text{cal}}+1)(1-\alpha)\rceil$. CP intervals are obtained by adding/subtracting
this radius to the MLP predictions on the raw scale. 





\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figure/BNN_vs_SplitCP.png}
    \caption{BNN and Split CP on UCI ENB2012.}
    \label{fig:bnn_vs_splitcp}
\end{figure}

Figure~\ref{fig:bnn_vs_splitcp} shows that, under the same MLP base model, BNN intervals deviate sharply from the ideal diagonal: the fraction of points outside the intervals is far above nominal, reflecting severely underestimated posterior variance and strong overconfidence. The intervals remain narrow across confidence levels, consistent with the observed undercoverage.

By contrast, Split Conformal closely follows the ideal line, with interval width increasing
predictably with confidence. Although wider, these intervals achieve near-nominal coverage.

Overall, BNN fails to provide valid frequentist coverage, while Split Conformal delivers stable, distribution-free, finite-sample guarantees.



\subsection{Boostrap resampling}
% Bootstrap residual intervals
% 残差方差估计偏小 → 区间过窄 → 覆盖率不足

% 优点：通用
% 缺点：
% 残差方差普遍估小 → 区间过窄
% 对 heavy-tail/noise 弱
% 对 out-of-domain 样本失效
% 无法做 object-wise calibration

% 1. 计算量巨大……
% 哇所以boostrap是在训练阶段要训练B次，预测阶段要调用B个模型，B 通常是 200–2000。
% Conformal prediction 只需要训练一次模型，预测阶段只需要计算 n+1 次非拟合度分数排序就可以了，计算量小很多。



% ## 总体设定（所有实验共用）

% * 回归模型：
%   (X \sim \text{Unif}[-1,1])，若干不同的真模型 (f^*) 与噪声 (\varepsilon)。
% * 目标：预测 (Y)，构造 95% 区间，比较：

%   * Conformal（split CP，NCM = |residual|）
%   * Bootstrap（residual bootstrap + 线性/NN 重拟合）
% * 评价：

%   * overall coverage：(\Pr{Y\in \hat C(X)})
%   * 平均区间长度
%   * 条件 coverage：在 x-bins 上估计 (\Pr{Y\in \hat C(X)\mid X\in \text{bin}})
%   * 运行时间（可选）

% 下面按 claim 设计 4 个实验。

% ---

% ## 实验 1：模型正确 vs 错误 → “Bootstrap 在 misspecification 下失效，CP 始终 valid”

% ### 1A 模型正确（线性 + 高斯噪声）

% * 真模型：
%   (Y = 2X + \varepsilon,\ \varepsilon \sim N(0,1))
% * 数据：

%   * train: n_train = 200
%   * cal: n_cal = 200（给 CP 用）
%   * test: n_test = 5000
% * 基模型：线性回归

% 方法：

% * CP：

%   * 在 train 上拟合线性模型，
%   * 在 cal 上算 residuals (|Y_i - \hat f(X_i)|)，取 (1−α)(n_cal+1)-th quantile = q_hat
%   * 区间：([\hat f(x) - q_{\text{hat}}, \hat f(x) + q_{\text{hat}}])
% * Bootstrap：

%   * 在 train 上拟合线性，得 residuals (\hat\varepsilon_i)
%   * 重抽 B=500 组 residuals，在每次 bootstrap 上重构 (Y^*)，重拟合线性，得到 (\hat f_b(x)) 或 residual 分布
%   * 取 bootstrap residual/预测分布的 2.5%–97.5% 形成区间

% 预期结果：

% * 两者的 coverage 都接近 0.95
% * Bootstrap 区间略短一点（高效），CP 稍长一点

% 这对应：**“模型正确时，Bayes/parametric/Bootstrap 比 CP 略更 efficient，但 CP 仍 valid”。**

% ---

% ### 1B 模型错误（非线性真模型 + 仍用线性）

% * 真模型：
%   (Y = 2X + 5X^2 + \varepsilon,\ \varepsilon \sim N(0,1))

% * 其它设定同上（train/cal/test）

% * 基模型：仍然用“线性回归”去拟合

% 方法同 1A。

% 预期现象：

% * CP：coverage 仍 ≈ 0.95，但 avg length 明显变长
% * Bootstrap：coverage 掉到比如 0.7–0.8，甚至更低，区间依然较短

% 这直接验证：

% > “随着假设模型越不对，Bootstrap 失去 validity 但保持短；CP 保持 validity 但区间变宽”。

% ---

% ## 实验 2：heavy-tail / 异方差噪声 → “Bootstrap 依赖分布条件，CP 只要 exchangeability”

% 设计两种噪声：

% ### 2A heavy-tail

% * 真模型：(Y = 2X + \varepsilon)
% * 噪声：(\varepsilon \sim t_3)（或 t_2）
% * 基模型仍线性回归

% ### 2B 异方差

% * 真模型：
%   (Y = 2X + \sigma(X)\varepsilon)
%   (\varepsilon \sim N(0,1))，(\sigma(X) = 0.5 + |X|)（靠近 ±1 方差大）

% 其它设定同前。

% 方法：

% * CP：像 1A，只是 residual 自然会反映 heavy-tail/heteroskedastic，quantile 变大
% * Bootstrap：仍用 residual bootstrap（很多人默认使用）

% 预期：

% * heavy-tail：

%   * CP coverage ≈ 0.95，区间很宽
%   * Bootstrap coverage 明显 <0.95，尤其 tail 部分 Y 被截断
% * 异方差：

%   * overall coverage 可以看起来还好，但**对大 |X| 的 bin**，Bootstrap coverage 会很低
%   * CP 的 conditional coverage 更平滑一些（虽然仍然只是 marginal 保证）

% 这验证：

% > “Bootstrap 对 heavy-tail/异方差敏感，finite-sample 下覆盖差；CP 只要 i.i.d./exchangeable 就有 finite-sample validity。”

% ---

% ## 实验 3：高维 + 黑箱模型 → “CP 可直接套在复杂估计器，Bootstrap 不稳”

% ### 设定

% * X 维度：p = 20

%   * 生成 X ~ N(0, Σ)，Σ_ij = 0.5^{|i-j|}
% * 真模型：
%   (Y = X_1 + X_2 + X_3 + \varepsilon,\ \varepsilon \sim N(0,1))
% * n_train = 300, n_cal = 300, n_test = 5000
% * 基模型：随机森林回归（或浅层 NN）

% 方法：

% * CP：

%   * 用 RF/NN 在 train 上拟合
%   * 在 cal 上 residual → quantile
% * Bootstrap：

%   * 对 train 做 residual bootstrap：每次重抽 residual + 重新训练 RF/NN（昂贵且不稳定）
%   * 用 bootstrap 预测分布取 2.5%–97.5%

% 记录：

% * coverage
% * length
% * 时间（train+区间构造）

% 预期：

% * high-dim + RF/NN 下：

%   * CP 基本能保持 coverage ~ 0.95（只要 base model 不完全爆炸）
%   * Bootstrap coverage 明显波动更大（模型拟合非常不稳定）
%   * 计算时间：Bootstrap >> CP（尤其有 CV/复杂模型时）

% 这支撑：

% > “在复杂/高维模型下，full parametric/Bootstrap 不再可靠，CP+split 非常实用且高效。”

% ---

% ## 实验 4：instance-wise vs global → “Bootstrap 给的是整体误差，CP 给单点置信信息”

% 选任意一个前面场景（比如 2B 异方差），在 test 上做：

% 1. 按 X 划分成若干 bin（例如 [-1,-0.5], [-0.5,0], [0,0.5], [0.5,1]）
% 2. 对每个 bin 计算：

%    * CP 的 coverage_bin
%    * Bootstrap 的 coverage_bin
%    * 两者平均区间长度

% 同时再看 “instance-wise 尺度”：

% * 对每个 test 点：

%   * CP：p-value 或 interval size (|C(x)|) 作为“不确定性指示器”
%   * Bootstrap：只能给一个“统一 nominal 95% 区间”，没有点级别置信度刻度

% 你可以画：

% * x-bin vs coverage（两条曲线：CP / Bootstrap）
% * x-bin vs mean interval length

% 预期：

% * Bootstrap 在大噪声区域（大 |X|）coverage 崩掉
% * CP 的 coverage 变化小得多
% * CP 自然给每个样本一个 interval size / p-value，展示 instance-wise 信心的“分辨率”；Bootstrap 的“95% 区间”在全局是统一标称，没有 per-instance 区别。

% ---

% ## 你论文里可以怎么用

% * Section “Experiments” 结构：

%   1. Linear-Gaussian（1A）：“sanity check: CP vs Bootstrap under correct model”
%   2. Nonlinear / heavy-tail / heteroskedastic（1B + 2）：“Bootstrap loses validity under misspecification; CP keeps finite-sample coverage, trades length”
%   3. High-dimensional RF/NN（3）：“CP remains valid and efficient on complex black-boxes, Bootstrap unstable and expensive”
%   4. Instance-wise analysis（4）：“Global vs instance-wise guarantees”

% 每个小节放一张表（coverage + length）+ 1–2 张关键图即可。







\subsection{quantile regression}
% 训练之后输出条件分位数，可以用这个来构建区间，但是无法保证覆盖率，也就是可信度时是下降的\cite{quantileregressionReview}
% 需要修改模型本身的loss function
% 但是现在有将两个结合起来的工作
% actually 现在有很多方法在把这两者结合起来


%需要做的实验有1. 验证模型错配情况下两种算法的表现，2给出有限样本下quantile regression的表现和conformalprediction的对比

\subsection{Others}
%随便写点吧，时间来不及就直接引用别人论文里面的内容







\section{Implications of a World Without Conformal Prediction}
TBC

\section{Conclusion}
TBC



\small
\bibliographystyle{unsrtnat}   % 按出现顺序排序
\bibliography{reference}     % 对应 references.bib 文件


\end{document}