{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "331cc8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Split Conformal (MLP) ===\n",
      "coverage: 0.8961038961038961\n",
      "avg_len: 2.3536224365234375\n",
      "q_hat: 0.11636930853128445\n",
      "\n",
      "=== Quantile Regression (MLP) ===\n",
      "coverage: 0.7987012987012987\n",
      "avg_len: 3.593290328979492\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# =========================\n",
    "# 0. 读数据 + 划分 + 标准化\n",
    "# =========================\n",
    "df = pd.read_excel(\"../../data/ENB2012_data.xlsx\")\n",
    "\n",
    "# 前 8 个特征，Y1: Heating Load\n",
    "X_raw = df.iloc[:, :8].values.astype(np.float32)\n",
    "y_raw = df.iloc[:, 8].values.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "# 60% 训练，20% 校准，20% 测试\n",
    "X_tr_raw, X_tmp_raw, y_tr_raw, y_tmp_raw = train_test_split(\n",
    "    X_raw, y_raw, test_size=0.4, random_state=0\n",
    ")\n",
    "X_cal_raw, X_test_raw, y_cal_raw, y_test_raw = train_test_split(\n",
    "    X_tmp_raw, y_tmp_raw, test_size=0.5, random_state=0\n",
    ")\n",
    "\n",
    "scaler_x = StandardScaler().fit(X_tr_raw)\n",
    "scaler_y = StandardScaler().fit(y_tr_raw)\n",
    "\n",
    "X_tr_np   = scaler_x.transform(X_tr_raw)\n",
    "X_cal_np  = scaler_x.transform(X_cal_raw)\n",
    "X_test_np = scaler_x.transform(X_test_raw)\n",
    "\n",
    "y_tr_np   = scaler_y.transform(y_tr_raw)\n",
    "y_cal_np  = scaler_y.transform(y_cal_raw)\n",
    "y_test    = scaler_y.transform(y_test_raw)  # 注意：y_test 保持 numpy，用于 inverse_transform\n",
    "\n",
    "# 转成 tensor（全部在 CPU 上）\n",
    "X_tr     = torch.tensor(X_tr_np,   dtype=torch.float32)\n",
    "y_tr     = torch.tensor(y_tr_np,   dtype=torch.float32)\n",
    "X_cal    = torch.tensor(X_cal_np,  dtype=torch.float32)\n",
    "y_cal    = torch.tensor(y_cal_np,  dtype=torch.float32)\n",
    "X_test_t = torch.tensor(X_test_np, dtype=torch.float32)\n",
    "\n",
    "# =========================\n",
    "# 2. 定义一个简单 MLP\n",
    "# =========================\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=8, hidden=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def train_mlp(X, y, epochs=200, batch_size=64, lr=1e-3):\n",
    "    ds = TensorDataset(X, y)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "    model = MLP(in_dim=X.shape[1])\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for xb, yb in dl:\n",
    "            opt.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# 3. Split Conformal (一次训练)\n",
    "# =========================\n",
    "def split_conformal(alpha=0.1):\n",
    "    model = train_mlp(X_tr, y_tr)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mu_cal = model(X_cal)\n",
    "    resid = (y_cal - mu_cal).abs().cpu().numpy().ravel()\n",
    "\n",
    "    # 用经验分位数估计 q_hat\n",
    "    q_hat = np.quantile(resid, 1 - alpha)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mu_test = model(X_test_t)\n",
    "    mu_test_np = mu_test.cpu().numpy().ravel()\n",
    "    lower = mu_test_np - q_hat\n",
    "    upper = mu_test_np + q_hat\n",
    "\n",
    "    # 还原回原尺度\n",
    "    y_test_np = scaler_y.inverse_transform(y_test).ravel()\n",
    "    mu_test_raw = scaler_y.inverse_transform(mu_test_np.reshape(-1, 1)).ravel()\n",
    "    lower_raw   = scaler_y.inverse_transform(lower.reshape(-1, 1)).ravel()\n",
    "    upper_raw   = scaler_y.inverse_transform(upper.reshape(-1, 1)).ravel()\n",
    "\n",
    "    coverage = np.mean((y_test_np >= lower_raw) & (y_test_np <= upper_raw))\n",
    "    avg_len  = np.mean(upper_raw - lower_raw)\n",
    "\n",
    "    return dict(\n",
    "        coverage=coverage,\n",
    "        avg_len=avg_len,\n",
    "        q_hat=q_hat,\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# 4. Quantile Regression (MLP)\n",
    "# =========================\n",
    "def quantile_loss(pred, y, tau):\n",
    "    err = y - pred\n",
    "    return torch.mean(torch.max(tau * err, (tau - 1) * err))\n",
    "\n",
    "def train_qr(X, y, tau=0.1, epochs=200, batch_size=64, lr=1e-3):\n",
    "    ds = TensorDataset(X, y)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "    model = MLP(in_dim=X.shape[1])\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for xb, yb in dl:\n",
    "            opt.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = quantile_loss(pred, yb, tau)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "    return model\n",
    "\n",
    "def quantile_regression(alpha=0.1):\n",
    "    tau_low  = alpha / 2.0        # 如 alpha=0.1 -> 0.05\n",
    "    tau_high = 1.0 - alpha / 2.0  # -> 0.95\n",
    "\n",
    "    # 分别训练下分位数和上分位数模型\n",
    "    model_low  = train_qr(X_tr, y_tr, tau=tau_low)\n",
    "    model_high = train_qr(X_tr, y_tr, tau=tau_high)\n",
    "\n",
    "    model_low.eval()\n",
    "    model_high.eval()\n",
    "    with torch.no_grad():\n",
    "        ql = model_low(X_test_t).cpu().numpy().ravel()\n",
    "        qu = model_high(X_test_t).cpu().numpy().ravel()\n",
    "\n",
    "    # 还原尺度\n",
    "    y_test_np = scaler_y.inverse_transform(y_test).ravel()\n",
    "    ql_raw = scaler_y.inverse_transform(ql.reshape(-1, 1)).ravel()\n",
    "    qu_raw = scaler_y.inverse_transform(qu.reshape(-1, 1)).ravel()\n",
    "\n",
    "    coverage = np.mean((y_test_np >= ql_raw) & (y_test_np <= qu_raw))\n",
    "    avg_len  = np.mean(qu_raw - ql_raw)\n",
    "\n",
    "    return dict(\n",
    "        coverage=coverage,\n",
    "        avg_len=avg_len,\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# 5. 运行对比\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    alpha = 0.1  # 90% 区间\n",
    "\n",
    "    res_cp = split_conformal(alpha=alpha)\n",
    "    res_qr = quantile_regression(alpha=alpha)\n",
    "\n",
    "    print(\"=== Split Conformal (MLP) ===\")\n",
    "    for k, v in res_cp.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    print(\"\\n=== Quantile Regression (MLP) ===\")\n",
    "    for k, v in res_qr.items():\n",
    "        print(f\"{k}: {v}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
